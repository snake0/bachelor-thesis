%# -*- coding: utf-8-unix -*-
\begin{bigabstract}

With the rapid development of data science and machine learning, the need for computing resources and memory resources grows dramatically. A scale-up single machine cannot meet the need for this huge request of resources. CPU clock rate is hard to be improved anymore. CPU architectures like Symmetric Multi-Processor and Non-Uniform Access Memory add more CPU cores to a single machine, but still fail to provide enough resources to new applications. Moreover, small businesses cannot afford a highly performant machine, and a highly performant machine induces the problem of system cooling and great power consumption. However, scale-out solutions, such as a distributed system, solve this problem in an effective and economic way. A large distributed system that is composed of hundreds and thousands of regular machines is able to provide enough resources to a large parallel task that needs a great number of CPU cores and a vast amount of memory. We name such a distributed system as a data center.

However, new environments introduce new problems. Firstly, software has to be rewritten to be able to run on a distributed environment, so porting single-machine software to the distributed environment can be a huge amount of work, or even unfeasible. For example, only software rewritten for the MapReduce framework can enjoy all the benefits of a MapReduce cluster. Giant Virtual Machine is a distributed hypervisor which spans across a cluster connected by network, and provides virtual hardware to the upper guest operating system. Giant Virtual Machine aggregates physical resources of several nodes in the cluster. As a result, the guest operating system running on it can enjoy the same benefits of the distributed system, without the need to be rewritten.

Secondly, poor CPU utilization has troubled data centers for decades and there has been a hot discussion of how to solve this problem. We observe that the root cause for CPU utilization to be low is that the need for CPU resources of a task may vary a lot during its runtime. For most of time it occupies a small amount of CPU, while the CPU usage can burst occasionally, approaching or even exceeding the total CPU capacity of the machine it runs on. Thus, the QoS of the task may suffer from not enough computing resources. If the task needs more resources than the capacity of the machine, we have no way to prevent its QoS from being affected because we have no more resources to fulfill its need.

In a cluster that has a huge number of machines, the underutilization of resources is mainly caused by the load imbalance among the cluster. Some machines in the cluster are fully occupied, while some other machines are idle. There are roughly three ways to solve the load imbalance problem. The first is to design a job scheduler. When a task becomes runnable, the task scheduler looks for the lowest loaded machine in the cluster and dispatches the task to it. This approach may in some way alleviate the load imbalance problem, but it fails to respond to the fluctuation of the work load on this single machine, since it only schedules the task to this machine and never reschedules it to another machine within the lifetime of the task. The second approach is to co-locate more tasks to a machine, which intuitively improves resource utilization. There are two sorts of tasks in a cluster. The latency critical tasks are sensitive to resource contention and should be allocated enough resources, while the best effort tasks are not that critical and resource contention does not cause a problem to it. As a result, to co-locate latency critical tasks with best effort tasks, we must employ a hardware or software resource isolation mechanism to ensure that resources of latency critical tasks are never preempted by best effort tasks. Co-location of tasks still fails to meet our requirement since amount of resource needed by tasks fluctuates over time and tasks have diverse workload patterns. 

In this work, we adopt the third approach, which is named resource reallocation, to solve the resource underutilization problem. By resource reallocation we mean migrating tasks between nodes with different loads in the cluster. The decision of migration is made by detecting resource interference at run time. If a machine is fully loaded, it may decide to migrate some of its tasks to another less loaded machine. The migration of tasks can be enabled by process level migration, container live migration, and virtual machine live migration. The three approaches to resource allocation suffer from their shortcomings respectively. Residual dependency caused by shared resources between tasks is the main impediment to the widespread use of process level migration in the industry and academy. Tasks that share opened files and shard memory structure with the source machine has no way to migrate those shared objects after being migrated to the destination machine. Container live migration confines the sharing between processes to a container and eliminates the residual dependency problem, but it incurs a fairly long down time for the migrated tasks. As the test result shows, the migration of a MySQL and Elasticsearch application with 250MB of memory in a container suffers from 2-3 seconds of down time.

Virtual machine live migration induces great network bandwidth consumption since we need to transfer every memory page of a virtual machine to resume the virtual machine correctly on the destination node. We suggest that the great network overhead introduced by virtual machine live migration is due to poor data locality of tasks being transferred in the virtual machine. As a task is migrated back and forward, additional guest OS memory pages are also transferred, which is unnecessary. We contend that a distributed hypervisor can help us to achieve good data locality, as a distributed hypervisor spans across several physical machines and the guest OS states are spread over the cluster by the distributed hypervisor, thus processes in the guest OS can be migrated among the cluster without migrating unnecessary memory pages of the guest OS, achieving high performance. 

We choose Giant Virtual Machine, which is an open-source distributed hypervisor that is based on the state-of-art Type-\uppercase\expandafter{\romannumeral2} virtual machine monitor QEMU-KVM, to verify our assumption. Giant Virtual Machine provides hardware that is of Non-uniform Memory Access architecture to the guest OS, since there is a similarity between NUMA and Distributed Shared Memory, a key component of Giant Virtual Machine. A dedicated scheduler in the guest OS is devised to dynamically detect host load by reading the /proc file system in the Linux system. The scheduler sets the affinity of all OS activities, including regular tasks, interrupts, and workqueues, to the lowest loaded NUMA node to make use of surplus resources in the machine and avoid excessive DSM pages faults, which can lead to too many network accesses and significant performance degradation. DSM implements Sequential Consistency, which is a fairly strong memory consistency model, so page thrashing problem arises if tasks in different NUMA nodes frequently access shared memory. As a compromise, we run all OS activities on one NUMA node at a time, which may slightly influence the performance tasks in the guest OS. As a result, we only run best effort tasks in guest OS of Giant Virtual Machine and co-locate latency critical workloads with Giant Virtual Machine to test whether a distributed hypervisor can improve CPU utilization in a cluster. Test data shows that comparing to vanilla QEMU-KVM, Giant Virtual Machine not only alleviates the QoS degradation of latency critical tasks when co-located with best effort tasks, but also effectively utilizes the surplus CPU resources in the cluster and improves the average CPU usage, and even guarantees the QoS of best effort tasks.

To demonstrate that our Giant Virtual Machine balancing mechanism has the ability to tackle the aforementioned problems, we simulate a cluster with 800 homogeneous machines by reading the trace data from Google Borg cluster and designing Task, Machine, Scheduler classes with Python scripts. Tasks are randomly scheduled to a Machine which has a CPU capacity of 0.5. The migrate function is invoked every ten timestamps, which simulates our Giant Virtual Machine balancing mechanism by migrating those migratable tasks to the lowest loaded node in every four nodes of the cluster, which is a G\_cell. Our simulation indicates that the Giant Virtual Machine balancing mechanism is capable of improving the latency of all sorts of tasks, dynamically rearranging workloads among the cluster, and finally alleviating load imbalance problem in a large data center.

We also observe that the scheduler can be finer-grained, which means that the second lowest loaded node of a G\_cell in the cluster is also an ideal destination for the guest OS to schedule its tasks to. We simulate the fine-grained scheduler in the same way as in the coarse-grained scheduler simulation. One Giant Virtual Machine instance is deployed on every four nodes, or the size of a G\_cell is set to 4. By setting a criterion for a task to be migratable, we get all the migratable tasks of a node, firstly schedule them to the lowest loaded node until it has not enough resources to hold more tasks, and then schedule the rest of tasks to the second lowest node until it has no more surplus resources. The test indicates that the fine-grained scheduler can effectively utilize the unused CPU resources in the second lowest node, and improve the resource utilization even more than the coarse-grained scheduler. In the meantime, the fine-grained scheduler also reduces a lot of network bandwidth consumption because it migrates fewer tasks in each migration iteration. Furthermore, we suggest that by sorting the tasks by its memory usage can minimize the network bandwidth consumption induced by the fine-grained migration. The Google cluster data mainly provides two data fields describing the memory status of tasks, which are Assigned Memory and Memory accesses Per Instruction, MPI. We run algorithms using different sorting criteria and give possible explanation to the test results. The test results show that reversely sorting the task by CPU usage reduces network bandwidth consumption the most, but leads to the lowest CPU utilization and the highest Gini coefficient. Sorting tasks by Assigned Memory Usage not only reduces network bandwidth consumption a lot, but also improves CPU utilization the most among all the sorting criteria. 


\end{bigabstract}