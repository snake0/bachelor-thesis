%# -*- coding: utf-8-unix -*-
\begin{bigabstract}

With the rapid development of data science and machine learning, the need for computing resources and memory resources grows dramatically. A scale-up single machine cannot meet the need for this huge request of resources. CPU clock rate is hard to be improved anymore. CPU architectures like Symmetric Multi-Processor and Non-Uniform Access Memory add more CPU cores to a single machine, but still fails to provide enough resources to new applications. Moreover, small businesses cannot afford a highly performant machine, and a highly performant machine induces the problem of system cooling and high power consumption. However, scale-out solutions, such as a distributed system, solves this problem in an effective and economic way. A large distributed system that is composed of hundreds and thousands of regular machines can provide enough resources to a highly parallel task that needs a great number of CPU cores and a vast amount of memory. 

However, new environment causes new problems. Firstly, software has to be rewritten to run on a distributed environment, which is a huge amount of work or even unfeasible. For example, only software rewritten for the MapReduce framework can enjoy all the benefits of the MapReduce cluster. Giant Virtual Machine is a distributed hypervisor which runs on a cluster connected by network, and provides virtual hardware to the upper guest operating system. Giant Virtual Machine aggregates the physical resources of several nodes in the cluster, thus the guest operating system that runs on Giant Virtual Machine monitor can enjoy the same benefits of the distributed system, without the need to rewrite the software.

Secondly, the CPU utilization is low and people have tried many different ways to improve CPU utilization in data centers. We observe that the root cause for CPU utilization to be low is that a task may occupy different amount of CPU resources during its runtime. CPU usage of a task may vary a lot as the task runs. For most of time it occupies a small amount of CPU, while the CPU usage can burst occasionally, approaching or even exceeding the total CPU capacity of the machine it runs on. Also, the QoS of the task may suffer from not enough computing resources. 

In a cluster that has a huge number of machines, the underutilization of resources is mainly caused by the load imbalance among the cluster. Some machines in the cluster is fully occupied, while some other machines are idle. There are three ways to solve the load imbalance problem. The first is to design a job scheduler. When a task becomes runnable, the task scheduler looks for the lowest loaded machine in the cluster and run the task on that machine. This approach may alleviate the load imbalance problem in some way, but it fails to respond to the fluctuation of work load, since it only schedules the task to a machine and never moves it to another machine within its lifetime. The second approach is to co-locate more tasks to a machine, which intuitively improve resource utilization. The latency critical tasks are sensitive to resource contention and should be allocated enough resources, while best effort tasks are not that critical and resource contention does not cause a problem. As a result, to co-locate latency critical tasks with best effort tasks, we must employ a hardware or software resource isolation mechanism to ensure that resources of latency critical tasks are never preempted. Co-location of tasks still fails to meet our requirement since amount of resource needed by tasks fluctuates over time and tasks have diverse workload patterns. 

In this work, we adopt the third approach, resource reallocation, to solve the underutilization problem. By resource reallocation we mean migrating tasks between differently loaded nodes in the cluster. The decision of migration is made by detecting resource interference at run time. If a machine is fully loaded, it may decide to migrate some of its tasks to another machine. The migration of tasks is enabled by process level migration, container live migration, or virtual machine live migration. The three approaches to resource allocation all suffer from their shortcomings. Residual dependency caused by resource sharing between tasks is the main impediment to the widespread use of process level migration in the industry and academy. Tasks that share opened files and shard memory structure with source machine has no way to migrate those shared objects after being migrated to the destination machine. Container live migration confines the sharing between processes in a container and eliminates the residual dependency problem, but it suffers from a long downtime. As data reported by the academy shows, the migration of a MySQL application with 250MB of memory in a container incurs 2-3 seconds of downtime.

Virtual machine live migration induces great network bandwidth consumption since we need to transfer every memory page of a virtual machine to resume the VM correctly on the destination node of the migration. We believe the great network overhead introduced by VM live migration is because the poor data locality of tasks being transferred in the virtual machine. As a task is migrated back and forth, additional guest OS memory pages are also transferred, which is not necessary. We contend that distributed hypervisor can achieve good data locality since a distributed hypervisor spans across several physical machines and guest OS states spread over the cluster the distributed hypervisor runs on, thus processes in the guest OS can migrate among the cluster without migrating unnecessary memory pages, achieving high performance. 

We use Giant Virtual Machine, which is an open-source distributed hypervisor based on the state-of-art Type-\uppercase\expandafter{\romannumeral2} virtual machine monitor, to verify our assumption. Giant Virtual Machine provides hardware of Non-uniform Memory Access architecture to the guest operating system, since there is a similarity between NUMA and Distributed Shared Memory, which is a key component of Giant Virtual Machine. A dedicated scheduler in the guest OS is devised to dynamically detect host load by reading the /proc file system in the Linux system. The scheduler sets the affinity of all OS activities, including regular tasks, interrupts, and workqueues, to the lowest loaded NUMA node to utilize the surplus resources in the cluster and avoid excessive DSM pages faults, which can lead to too many network accesses and performance degradation. DSM implements Sequential Consistency, which is a fairly strong memory consistency model, and page thrashing problem arises if tasks in different NUMA nodes frequently access shared memory. As a compromise, we run all OS activities on one NUMA node at a time, which may slightly reduce the QoS of best effort tasks. We run best effort tasks in guest OS of Giant Virtual Machine and co-locates latency critical workloads with Giant Virtual Machine to test whether a distributed hypervisor can improve CPU utilization in a cluster. Test shows that comparing to vanilla virtual machines, Giant Virtual Machine not only alleviates the QoS violation of latency critical tasks when co-located with best effort tasks, but also effectively utilize the surplus CPU resources in the cluster and improve the average CPU usage, and even QoS of best effort tasks is guaranteed.

To demonstrate that our Giant Virtual Machine balancing mechanism has the ability to tackle the aforementioned problems, we simulate a cluster with 800 homogeneous machines by reading the trace data from Google Borg cluster and designing Task, Machine, Scheduler classes with Python. Tasks are randomly scheduled to a Machine which has a CPU capacity of 0.5. The migrate function is invoked every ten timestamps, which simulates our Giant Virtual Machine balancing mechanism by migrating those migratable tasks to the lowest loaded node in every four nodes of the cluster. Simulation indicates that our balancing mechanism is capable of alleviating load imbalance in the cluster, improving the latency of tasks, and dynamically rearranging workloads among the cluster. 

We also observe that the scheduler can be finer-grained, which means the second lowest loaded node of every four nodes in the cluster is also an ideal destination for guest OS to schedule its tasks to. We simulate the fine-grained scheduler also in the same way as in the coarse-grained scheduler simulation. One Giant Virtual Machine instance is deployed on every four nodes. By setting a criterion for a task to be migratable, we get all the migratable tasks of a node, and first schedule them to the lowest loaded node until it cannot hold more tasks, then schedule the rest of them to the second lowest node. The test demonstrates that the fine-grained scheduler can effectively utilize the unused CPU resources in the second lowest node, improving the resource utilization even more than the coarse-grained scheduler. In the meantime, the fine-grained scheduler also reduces network bandwidth consumption because it migrates fewer tasks every migration iteration. Furthermore, we suggest that by sorting the tasks by their memory usage can minimize the network bandwidth consumption induced by the fine-grained migration. The Google cluster data mainly provides two data fields describing the memory status of tasks, Assigned Memory and Memory accesses Per Instruction. We compare different sorting criteria and give possible explanation to the test data. The test results show that reversely sorting the task by CPU usage reduces network bandwidth consumption the best, but leads to the worst CPU utilization and Gini coefficient. Sorting by Assigned Memory Usage not only reduces network bandwidth consumption very well, but also improves CPU utilization the most among all the sorting criteria. 


\end{bigabstract}